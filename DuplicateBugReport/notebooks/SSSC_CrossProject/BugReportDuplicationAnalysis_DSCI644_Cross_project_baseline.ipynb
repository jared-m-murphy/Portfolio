{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "c99EvWo1s9-x",
    "outputId": "fef3ad50-d0e1-4b1d-9782-a05d1e9d2047"
   },
   "outputs": [],
   "source": [
    "# Loading all required libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Embedding, LSTM, GRU, Conv1D, Conv2D, GlobalMaxPool1D, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# This will prompt for authorization.\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "EEudw61-ld6r"
   },
   "outputs": [],
   "source": [
    "#Tokeniztion and stop word removal function\n",
    "def tokenization(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "preprocessed = lambda x: transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "H4SJ-tGNkOeY"
   },
   "outputs": [],
   "source": [
    "#All the data files from the various projects\n",
    "\n",
    "#Eclipse BugReports\n",
    "Eclip_Non_Dup = './dataBR/Eclipse/EP_nondup.csv'\n",
    "Eclip_Dup = './dataBR/Eclipse/EP_dup.csv'\n",
    "\n",
    "\n",
    "#Mozilla BugReports\n",
    "Mozilla_Non_Dup = './dataBR/Mozilla/M_NonDuplicate.csv'\n",
    "Mozilla_Dup = './dataBR/Mozilla/M_Duplicate.csv'\n",
    "\n",
    "#ThunderBird Bug Reports\n",
    "ThunderBird_Non_Dup = './dataBR/ThunderBird/Nondup_TB.csv'\n",
    "ThunderBird_Dup = './dataBR/ThunderBird/dup_TB.csv'\n",
    "\n",
    "\n",
    "#Eclipse dataframes\n",
    "Eclipse_dups = pd.read_csv(Eclip_Dup,sep=\";\", engine='python')\n",
    "Eclipse_nondups = pd.read_csv(Eclip_Non_Dup,sep=\";\", engine='python')\n",
    "Eclipse_combined = pd.concat([Eclipse_dups, Eclipse_nondups], ignore_index=True, sort=False)\n",
    "Eclipse_combined['Report1'] = Eclipse_combined['Title1'] +\" \"+ Eclipse_combined['Description1']\n",
    "Eclipse_combined['Report2'] = Eclipse_combined['Title2'] +\" \"+ Eclipse_combined['Description2']\n",
    "\n",
    "\n",
    "#Mozilla dataframes\n",
    "Mozilla_dups = pd.read_csv(Mozilla_Dup,sep=\";\", engine='python')\n",
    "Mozilla_nondups = pd.read_csv(Mozilla_Non_Dup,sep=\";\", engine='python')\n",
    "Mozilla_combined = pd.concat([Mozilla_dups, Mozilla_nondups], ignore_index=True, sort=False)\n",
    "Mozilla_combined['Report1'] = Mozilla_combined['Title1'] +\" \"+ Mozilla_combined['Description1']\n",
    "Mozilla_combined['Report2'] = Mozilla_combined['Title2'] +\" \"+ Mozilla_combined['Description2']\n",
    "\n",
    "\n",
    "#ThunderBird dataframes\n",
    "ThunderBird_dups = pd.read_csv(ThunderBird_Dup,sep=\";\", engine='python')\n",
    "ThunderBird_nondups = pd.read_csv(ThunderBird_Non_Dup,sep=\";\", engine='python')\n",
    "ThunderBird_combined = pd.concat([ThunderBird_dups, ThunderBird_nondups], ignore_index=True, sort=False)\n",
    "ThunderBird_combined['Report1'] = ThunderBird_combined['Title1'] +\" \"+ ThunderBird_combined['Description1']\n",
    "ThunderBird_combined['Report2'] = ThunderBird_combined['Title2'] +\" \"+ ThunderBird_combined['Description2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eE21tD7TlSSS",
    "outputId": "1f256f5f-2b9b-4119-8458-26877ba2e207"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    34222\n",
       "1    12686\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking distribution of Label column\n",
    "Eclipse_combined.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "5NgRbNmq5g_m",
    "outputId": "3ccaf8cb-0d46-4ce5-c5d7-5fac84e427a3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train, test and validation splits for all 3 projects\n",
    "#Validation and Test frames are of equal size : 20% of overall dataset each\n",
    "\n",
    "Train_Eclipse_intermediate, Test_Eclipse = train_test_split(Eclipse_combined, test_size=0.2, random_state=42)\n",
    "Train_Eclipse, Validation_Eclipse = train_test_split(Train_Eclipse_intermediate, test_size=0.25, random_state=56)\n",
    "\n",
    "Train_Mozilla_intermediate, Test_Mozilla = train_test_split(Mozilla_combined, test_size=0.2, random_state=78)\n",
    "Train_Mozilla, Validation_Mozilla = train_test_split(Train_Mozilla_intermediate, test_size=0.25, random_state=56)\n",
    "\n",
    "Train_ThunderBird_intermediate, Test_intermediate = train_test_split(ThunderBird_combined, test_size=0.20, random_state=98)\n",
    "Train_ThunderBird, Validation_ThunderBird = train_test_split(Train_ThunderBird_intermediate, test_size=0.25, random_state=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running word2vec.py to build vocabulary with Google News dataset and all bug reports in project\n",
    "#Run only once\n",
    "\n",
    "\n",
    "#%run -i 'word2vec.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec model(it may takes 2-3 mins) ...\n",
      "WORD2VEC loaded\n",
      "1,000 sentences embedded.\n",
      "2,000 sentences embedded.\n",
      "3,000 sentences embedded.\n",
      "4,000 sentences embedded.\n",
      "5,000 sentences embedded.\n",
      "6,000 sentences embedded.\n",
      "7,000 sentences embedded.\n",
      "8,000 sentences embedded.\n",
      "9,000 sentences embedded.\n",
      "10,000 sentences embedded.\n",
      "11,000 sentences embedded.\n",
      "12,000 sentences embedded.\n",
      "13,000 sentences embedded.\n",
      "14,000 sentences embedded.\n",
      "15,000 sentences embedded.\n",
      "16,000 sentences embedded.\n",
      "17,000 sentences embedded.\n",
      "18,000 sentences embedded.\n",
      "19,000 sentences embedded.\n",
      "20,000 sentences embedded.\n",
      "21,000 sentences embedded.\n",
      "22,000 sentences embedded.\n",
      "23,000 sentences embedded.\n",
      "24,000 sentences embedded.\n",
      "25,000 sentences embedded.\n",
      "26,000 sentences embedded.\n",
      "27,000 sentences embedded.\n",
      "28,000 sentences embedded.\n",
      "29,000 sentences embedded.\n",
      "30,000 sentences embedded.\n",
      "31,000 sentences embedded.\n",
      "32,000 sentences embedded.\n",
      "33,000 sentences embedded.\n",
      "34,000 sentences embedded.\n",
      "35,000 sentences embedded.\n",
      "36,000 sentences embedded.\n",
      "37,000 sentences embedded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 19:43:59.771390: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 50)           57562500    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " man_dist (ManDist)             (None, 1)            0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 57,562,500\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 57,492,300\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 8s 377ms/step - loss: 0.2638 - accuracy: 0.7360 - val_loss: 0.2618 - val_accuracy: 0.7380\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 5s 329ms/step - loss: 0.2634 - accuracy: 0.7362 - val_loss: 0.2617 - val_accuracy: 0.7381\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 5s 344ms/step - loss: 0.2628 - accuracy: 0.7367 - val_loss: 0.2611 - val_accuracy: 0.7382\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 5s 385ms/step - loss: 0.2602 - accuracy: 0.7371 - val_loss: 0.2557 - val_accuracy: 0.7377\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 6s 417ms/step - loss: 0.2533 - accuracy: 0.7368 - val_loss: 0.2488 - val_accuracy: 0.7351\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 6s 449ms/step - loss: 0.2456 - accuracy: 0.7391 - val_loss: 0.2432 - val_accuracy: 0.7359\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 7s 509ms/step - loss: 0.2378 - accuracy: 0.7446 - val_loss: 0.2379 - val_accuracy: 0.7357\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 8s 554ms/step - loss: 0.2304 - accuracy: 0.7480 - val_loss: 0.2326 - val_accuracy: 0.7377\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 8s 583ms/step - loss: 0.2229 - accuracy: 0.7536 - val_loss: 0.2281 - val_accuracy: 0.7388\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 8s 589ms/step - loss: 0.2160 - accuracy: 0.7578 - val_loss: 0.2250 - val_accuracy: 0.7389\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 8s 589ms/step - loss: 0.2101 - accuracy: 0.7634 - val_loss: 0.2219 - val_accuracy: 0.7401\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 8s 557ms/step - loss: 0.2048 - accuracy: 0.7683 - val_loss: 0.2199 - val_accuracy: 0.7392\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 8s 551ms/step - loss: 0.2000 - accuracy: 0.7726 - val_loss: 0.2179 - val_accuracy: 0.7407\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 7s 501ms/step - loss: 0.1956 - accuracy: 0.7774 - val_loss: 0.2158 - val_accuracy: 0.7415\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 7s 485ms/step - loss: 0.1916 - accuracy: 0.7816 - val_loss: 0.2144 - val_accuracy: 0.7406\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 6s 453ms/step - loss: 0.1879 - accuracy: 0.7855 - val_loss: 0.2128 - val_accuracy: 0.7404\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 6s 444ms/step - loss: 0.1846 - accuracy: 0.7880 - val_loss: 0.2117 - val_accuracy: 0.7396\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 6s 445ms/step - loss: 0.1810 - accuracy: 0.7918 - val_loss: 0.2102 - val_accuracy: 0.7421\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 6s 446ms/step - loss: 0.1781 - accuracy: 0.7948 - val_loss: 0.2088 - val_accuracy: 0.7440\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 6s 444ms/step - loss: 0.1748 - accuracy: 0.7977 - val_loss: 0.2076 - val_accuracy: 0.7430\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 6s 438ms/step - loss: 0.1718 - accuracy: 0.8009 - val_loss: 0.2069 - val_accuracy: 0.7416\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 6s 451ms/step - loss: 0.1691 - accuracy: 0.8035 - val_loss: 0.2057 - val_accuracy: 0.7410\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 6s 454ms/step - loss: 0.1664 - accuracy: 0.8068 - val_loss: 0.2048 - val_accuracy: 0.7405\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 6s 467ms/step - loss: 0.1638 - accuracy: 0.8107 - val_loss: 0.2041 - val_accuracy: 0.7398\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 7s 469ms/step - loss: 0.1614 - accuracy: 0.8123 - val_loss: 0.2032 - val_accuracy: 0.7398\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 7s 474ms/step - loss: 0.1591 - accuracy: 0.8146 - val_loss: 0.2026 - val_accuracy: 0.7378\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 6s 467ms/step - loss: 0.1568 - accuracy: 0.8193 - val_loss: 0.2022 - val_accuracy: 0.7398\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 7s 479ms/step - loss: 0.1546 - accuracy: 0.8205 - val_loss: 0.2015 - val_accuracy: 0.7390\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 6s 467ms/step - loss: 0.1525 - accuracy: 0.8244 - val_loss: 0.2010 - val_accuracy: 0.7394\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 7s 470ms/step - loss: 0.1505 - accuracy: 0.8263 - val_loss: 0.2006 - val_accuracy: 0.7399\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 7s 493ms/step - loss: 0.1487 - accuracy: 0.8283 - val_loss: 0.2002 - val_accuracy: 0.7404\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 7s 497ms/step - loss: 0.1469 - accuracy: 0.8329 - val_loss: 0.2000 - val_accuracy: 0.7381\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 7s 473ms/step - loss: 0.1452 - accuracy: 0.8343 - val_loss: 0.1995 - val_accuracy: 0.7401\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 7s 470ms/step - loss: 0.1437 - accuracy: 0.8353 - val_loss: 0.1995 - val_accuracy: 0.7406\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 7s 470ms/step - loss: 0.1419 - accuracy: 0.8391 - val_loss: 0.1991 - val_accuracy: 0.7400\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 6s 469ms/step - loss: 0.1404 - accuracy: 0.8402 - val_loss: 0.1990 - val_accuracy: 0.7385\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 6s 457ms/step - loss: 0.1388 - accuracy: 0.8446 - val_loss: 0.1987 - val_accuracy: 0.7396\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 6s 447ms/step - loss: 0.1373 - accuracy: 0.8464 - val_loss: 0.1986 - val_accuracy: 0.7397\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 6s 447ms/step - loss: 0.1361 - accuracy: 0.8469 - val_loss: 0.1981 - val_accuracy: 0.7414\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 6s 445ms/step - loss: 0.1346 - accuracy: 0.8502 - val_loss: 0.1982 - val_accuracy: 0.7390\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 7s 472ms/step - loss: 0.1332 - accuracy: 0.8515 - val_loss: 0.1976 - val_accuracy: 0.7407\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 6s 463ms/step - loss: 0.1319 - accuracy: 0.8532 - val_loss: 0.1976 - val_accuracy: 0.7398\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 7s 472ms/step - loss: 0.1306 - accuracy: 0.8557 - val_loss: 0.1972 - val_accuracy: 0.7390\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 7s 482ms/step - loss: 0.1293 - accuracy: 0.8577 - val_loss: 0.1975 - val_accuracy: 0.7390\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 7s 518ms/step - loss: 0.1280 - accuracy: 0.8597 - val_loss: 0.1970 - val_accuracy: 0.7404\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 7s 516ms/step - loss: 0.1268 - accuracy: 0.8610 - val_loss: 0.1970 - val_accuracy: 0.7399\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 7s 516ms/step - loss: 0.1259 - accuracy: 0.8633 - val_loss: 0.1974 - val_accuracy: 0.7382\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 7s 522ms/step - loss: 0.1246 - accuracy: 0.8662 - val_loss: 0.1971 - val_accuracy: 0.7397\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 7s 494ms/step - loss: 0.1235 - accuracy: 0.8688 - val_loss: 0.1968 - val_accuracy: 0.7413\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 7s 505ms/step - loss: 0.1222 - accuracy: 0.8696 - val_loss: 0.1968 - val_accuracy: 0.7399\n",
      "Training time finished.\n",
      "50 epochs in       331.49\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load training set and resetting index\n",
    "train_df = Train_Eclipse_intermediate.reset_index()\n",
    "for q in ['Report1', 'Report2']:\n",
    "    train_df[q + '_n'] = train_df[q]\n",
    "\n",
    "# Make word2vec embeddings\n",
    "embedding_dim = 300\n",
    "max_seq_length = 20\n",
    "use_w2v = False\n",
    "\n",
    "#trigerring make_w2v_embeddings support function from 'util.py' for the word embedding and vectorization process\n",
    "#detailed in the final Paper submission\n",
    "train_df, embeddings = make_w2v_embeddings(train_df, embedding_dim=embedding_dim, empty_w2v=use_w2v)\n",
    "\n",
    "# Split to train validation, assigning size\n",
    "X = train_df[['Report1_n','Report2_n']]\n",
    "Y = train_df['Label']\n",
    "\n",
    "#Train and validation split\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "#trigerring split_and_zero_padding support function from 'util.py' for both train and validation dataframes\n",
    "X_train = split_and_zero_padding(X_train, max_seq_length)\n",
    "X_validation = split_and_zero_padding(X_validation, max_seq_length)\n",
    "\n",
    "# Convert labels to their numpy representations\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "# Make sure shape of dataframe is as expected\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)\n",
    "\n",
    "# -- BEGIN MODEL ----\n",
    "\n",
    "# Model variables\n",
    "gpus = 2\n",
    "batch_size = 1024 * gpus\n",
    "n_epoch = 50\n",
    "n_hidden = 50\n",
    "\n",
    "# Define the shared model\n",
    "x = Sequential()\n",
    "#using word2vec generated embeddings\n",
    "x.add(Embedding(len(embeddings), embedding_dim,\n",
    "                weights=[embeddings], input_shape=(max_seq_length,), trainable=False))\n",
    "#Adding the LSTM layer for the Siamese Signal Subnet Compression\n",
    "x.add(LSTM(n_hidden))\n",
    "shared_model = x\n",
    "\n",
    "# The visible layer\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "# Rolled up into a Manhattan Distance model\n",
    "malstm_distance = ManDist()([shared_model(left_input), shared_model(right_input)])\n",
    "model = Model(inputs=[left_input, right_input], outputs=[malstm_distance])\n",
    "\n",
    "# Build model with MSE loss, Adam optimizer and optimizing for accuracy \n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Display Model Summary\n",
    "model.summary()\n",
    "\n",
    "# Start training over the epochs\n",
    "training_start_time = time()\n",
    "malstm_trained = model.fit([X_train['left'], X_train['right']], Y_train,\n",
    "                           batch_size=batch_size, epochs=n_epoch,\n",
    "                           validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
    "training_end_time = time()\n",
    "\n",
    "print(\"Training time finished.\\n%d epochs in %12.2f\" % (n_epoch,\n",
    "                                                        training_end_time - training_start_time))\n",
    "#Saving the model to a file for inference \n",
    "model.save('./data/BugClassifierLSTM.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec model(it may takes 2-3 mins) ...\n",
      "WORD2VEC loaded\n",
      "1,000 sentences embedded.\n",
      "2,000 sentences embedded.\n",
      "3,000 sentences embedded.\n",
      "4,000 sentences embedded.\n",
      "5,000 sentences embedded.\n",
      "6,000 sentences embedded.\n",
      "7,000 sentences embedded.\n",
      "8,000 sentences embedded.\n",
      "9,000 sentences embedded.\n",
      "10,000 sentences embedded.\n",
      "11,000 sentences embedded.\n",
      "12,000 sentences embedded.\n",
      "13,000 sentences embedded.\n",
      "14,000 sentences embedded.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 50)           57562500    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " man_dist (ManDist)             (None, 1)            0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 57,562,500\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 57,492,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Loading libraries for test run\n",
    "from util import make_w2v_embeddings\n",
    "from util import split_and_zero_padding\n",
    "from util import ManDist\n",
    "\n",
    "# Load testing set\n",
    "y_test = ThunderBird_combined['Label'].values\n",
    "test_df = ThunderBird_combined\n",
    "for q in ['Report1', 'Report2']:\n",
    "    test_df[q + '_n'] = test_df[q]\n",
    "\n",
    "# Make word2vec embeddings on test\n",
    "embedding_dim = 300\n",
    "max_seq_length = 20\n",
    "test_df, embeddings = make_w2v_embeddings(test_df, embedding_dim=embedding_dim, empty_w2v=False)\n",
    "\n",
    "# Split to dicts and append zero padding.\n",
    "X_test = split_and_zero_padding(test_df, max_seq_length)\n",
    "\n",
    "# Make sure everything is ok\n",
    "assert X_test['left'].shape == X_test['right'].shape\n",
    "\n",
    "# --\n",
    "\n",
    "model = keras.models.load_model('./data/BugClassifierLSTM.h5', custom_objects={'ManDist': ManDist})\n",
    "model.summary()\n",
    "\n",
    "y_test_pred = model.predict([X_test['left'], X_test['right']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "#Making a prediction on across different project dataset (testdataset)\n",
    "threshold = 0.1\n",
    "Y_pred_final = np.where(y_test_pred > threshold, 1,0)\n",
    "print(Y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC_score of model :  0.519858168433065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.58      0.64      9905\n",
      "           1       0.33      0.46      0.38      4358\n",
      "\n",
      "    accuracy                           0.54     14263\n",
      "   macro avg       0.52      0.52      0.51     14263\n",
      "weighted avg       0.59      0.54      0.56     14263\n",
      "\n",
      "[[5755 4150]\n",
      " [2359 1999]]\n"
     ]
    }
   ],
   "source": [
    "#Printing the performance of results obtained from model on the test set\n",
    "#For the dataset selected for evaluation above and at specified threshold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print(\"ROC_AUC_score of model : \",roc_auc_score(y_test, Y_pred_final))\n",
    "print(classification_report(y_test, Y_pred_final))\n",
    "print(confusion_matrix(y_test, Y_pred_final))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BugReportDuplicationAnalysis_DSCI644_v1.0.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
